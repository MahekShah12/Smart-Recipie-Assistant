{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4810ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 346\u001b[39m\n\u001b[32m    343\u001b[39m np.random.seed(RANDOM_SEED)\n\u001b[32m    345\u001b[39m df_all = load_all_datasets(DATASETS)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m model = \u001b[43mHybridSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    348\u001b[39m metrics = evaluate_model(model, top_k=TOP_K_DEFAULT, num_queries=\u001b[32m200\u001b[39m)\n\u001b[32m    349\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluation (synthetic retrieval):\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 231\u001b[39m, in \u001b[36mHybridSearch.fit\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    228\u001b[39m ing_corpus = [r.ing_text \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.records]\n\u001b[32m    229\u001b[39m ins_corpus = [r.ins_text \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.records]\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[38;5;28mself\u001b[39m.M_ing_word = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ming_word\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43ming_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28mself\u001b[39m.M_ing_char = \u001b[38;5;28mself\u001b[39m.ing_char.fit_transform(ing_corpus)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mlen\u001b[39m(t) > \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ins_corpus):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[39m, in \u001b[36mTfidfVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._check_params()\n\u001b[32m   2098\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf = TfidfTransformer(\n\u001b[32m   2099\u001b[39m     norm=\u001b[38;5;28mself\u001b[39m.norm,\n\u001b[32m   2100\u001b[39m     use_idf=\u001b[38;5;28mself\u001b[39m.use_idf,\n\u001b[32m   2101\u001b[39m     smooth_idf=\u001b[38;5;28mself\u001b[39m.smooth_idf,\n\u001b[32m   2102\u001b[39m     sublinear_tf=\u001b[38;5;28mself\u001b[39m.sublinear_tf,\n\u001b[32m   2103\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m X = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[38;5;28mself\u001b[39m._tfidf.fit(X)\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1368\u001b[39m             warnings.warn(\n\u001b[32m   1369\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1372\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1373\u001b[39m             )\n\u001b[32m   1374\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1379\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mahek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1280\u001b[39m     vocabulary = \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1284\u001b[39m         )\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indptr[-\u001b[32m1\u001b[39m] > np.iinfo(np.int32).max:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[31mValueError\u001b[39m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- ML / NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# --- GUI\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "\n",
    "# --- Stemming (no external downloads needed)\n",
    "try:\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    STEMMER = SnowballStemmer(\"english\")\n",
    "except Exception:\n",
    "    STEMMER = None  # Fallback (skip stemming if not available)\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "DATASETS = [\n",
    "    # r\"C:\\users\\mahek\\Desktop\\pbl5\\indian_food.csv\",\n",
    "    # r\"C:\\users\\mahek\\Desktop\\pbl5\\Food_Recipe.csv\",\n",
    "    # r\"C:\\users\\mahek\\Desktop\\pbl5\\recipes.csv\",\n",
    "    r\"C:\\Users\\mahek\\Desktop\\pbl5\\indian_recipes.csv\"\n",
    "]\n",
    "TOP_K_DEFAULT = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Ingredient-synonym/alias mapping\n",
    "SYNONYM_MAP_PHRASES = {\n",
    "    r\"\\bcapsicum\\b\": \"bell pepper\",\n",
    "    r\"\\bmirchi\\b\": \"chili\",\n",
    "    r\"\\bchilli\\b\": \"chili\",\n",
    "    r\"\\bchilies\\b\": \"chili\",\n",
    "    r\"\\bgreen chili(es)?\\b\": \"chili\",\n",
    "    r\"\\bred chili(es)?\\b\": \"chili\",\n",
    "    r\"\\bcoriander leaves\\b\": \"coriander\",\n",
    "    r\"\\bdhania\\b\": \"coriander\",\n",
    "    r\"\\bcilantro\\b\": \"coriander\",\n",
    "    r\"\\bjeera\\b\": \"cumin\",\n",
    "    r\"\\bzeera\\b\": \"cumin\",\n",
    "    r\"\\bhari mirch\\b\": \"chili\",\n",
    "    r\"\\bhing\\b\": \"asafoetida\",\n",
    "    r\"\\bgur\\b\": \"jaggery\",\n",
    "    r\"\\bgud\\b\": \"jaggery\",\n",
    "    r\"\\bmaida\\b\": \"all purpose flour\",\n",
    "    r\"\\batta\\b\": \"wheat flour\",\n",
    "    r\"\\bdahi\\b\": \"yogurt\",\n",
    "    r\"\\bcurd\\b\": \"yogurt\",\n",
    "    r\"\\bmethi\\b\": \"fenugreek\",\n",
    "    r\"\\bkasuri methi\\b\": \"fenugreek\",\n",
    "    r\"\\baloo\\b\": \"potato\",\n",
    "    r\"\\bbhindi\\b\": \"okra\",\n",
    "    r\"\\bturka\\b\": \"tempering\",\n",
    "}\n",
    "\n",
    "# Common noise words\n",
    "NOISE_TOKENS = {\n",
    "    \"fresh\", \"finely\", \"chopped\", \"sliced\", \"diced\", \"ground\", \"powder\",\n",
    "    \"whole\", \"optional\", \"to\", \"taste\", \"medium\", \"large\", \"small\", \"cup\",\n",
    "    \"cups\", \"tsp\", \"tbsp\", \"tablespoon\", \"teaspoon\", \"pinch\", \"piece\",\n",
    "    \"pieces\", \"handful\", \"and\", \"or\", \"of\", \"oil\", \"water\"\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Loading & Unification\n",
    "# =========================\n",
    "def load_dataset(path: Union[str, Path]) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {path}\")\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "    elif path.suffix.lower() == \".json\":\n",
    "        df = pd.read_json(path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Provide CSV or JSON.\")\n",
    "    df.columns = [re.sub(r\"\\s+\", \"_\", c.strip().lower()) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "def unify_dataset(df: pd.DataFrame, src_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map dataset-specific columns to a unified schema: [name, ingredients, instructions]\n",
    "    \"\"\"\n",
    "    name_col = None\n",
    "    ing_col = None\n",
    "    ins_col = None\n",
    "\n",
    "    if \"indian_food\" in src_name.lower():\n",
    "        name_col = \"name\"\n",
    "        ing_col = \"ingredients\"\n",
    "        ins_col = None  # no instructions\n",
    "\n",
    "    elif \"food_recipe\" in src_name.lower():\n",
    "        name_col = \"recipe_name\"\n",
    "        ing_col = \"ingredients\"\n",
    "        ins_col = \"directions\"\n",
    "\n",
    "    elif \"recipes\" in src_name.lower():\n",
    "        name_col = \"name\"\n",
    "        ing_col = \"ingredients_name\"\n",
    "        ins_col = \"instructions\"\n",
    "\n",
    "    # fallback detection\n",
    "    if not name_col:\n",
    "        for c in [\"name\", \"title\", \"recipe_name\", \"dish\", \"recipe\"]:\n",
    "            if c in df.columns:\n",
    "                name_col = c\n",
    "                break\n",
    "    if not ing_col:\n",
    "        for c in [\"ingredients\", \"ingredient\", \"ingredients_name\", \"translatedingredients\"]:\n",
    "            if c in df.columns:\n",
    "                ing_col = c\n",
    "                break\n",
    "    if not ins_col:\n",
    "        for c in [\"instructions\", \"steps\", \"directions\", \"method\", \"translatedinstructions\"]:\n",
    "            if c in df.columns:\n",
    "                ins_col = c\n",
    "                break\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"name\": df[name_col] if name_col in df.columns else [f\"recipe_{i}\" for i in range(len(df))],\n",
    "        \"ingredients\": df[ing_col] if ing_col in df.columns else \"\",\n",
    "        \"instructions\": df[ins_col] if ins_col and ins_col in df.columns else \"\",\n",
    "    })\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_all_datasets(paths: List[str]) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        df = load_dataset(p)\n",
    "        df_u = unify_dataset(df, p)\n",
    "        dfs.append(df_u)\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    return df_all\n",
    "\n",
    "# =========================\n",
    "# Normalization helpers\n",
    "# =========================\n",
    "def apply_synonyms(text: str) -> str:\n",
    "    s = text\n",
    "    for pattern, repl in SYNONYM_MAP_PHRASES.items():\n",
    "        s = re.sub(pattern, repl, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def tokenize_items(value: Any) -> List[str]:\n",
    "    if pd.isna(value):\n",
    "        return []\n",
    "    if isinstance(value, (list, tuple, set)):\n",
    "        items = list(value)\n",
    "    else:\n",
    "        items = re.split(r\"[,;\\|\\n]+\", str(value))\n",
    "    toks: List[str] = []\n",
    "    for item in items:\n",
    "        s = item.lower().strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        s = apply_synonyms(s)\n",
    "        s = re.sub(r\"(\\d+\\/\\d+|\\d+\\.\\d+|\\d+)\", \" \", s)\n",
    "        s = re.sub(r\"[^a-z\\s\\-]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        for t in s.split():\n",
    "            if t in NOISE_TOKENS:\n",
    "                continue\n",
    "            if STEMMER:\n",
    "                t = STEMMER.stem(t)\n",
    "            toks.append(t)\n",
    "    return toks\n",
    "\n",
    "\n",
    "def normalize_ingredients(value: Any) -> str:\n",
    "    return \" \".join(tokenize_items(value))\n",
    "\n",
    "\n",
    "def normalize_instructions(value: Any) -> str:\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    s = str(value).lower()\n",
    "    s = apply_synonyms(s)\n",
    "    s = re.sub(r\"[^a-z\\s\\-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    if STEMMER:\n",
    "        s = \" \".join(STEMMER.stem(t) for t in s.split() if t not in NOISE_TOKENS)\n",
    "    return s\n",
    "\n",
    "# =========================\n",
    "# Model\n",
    "# =========================\n",
    "class RecipeRecord:\n",
    "    def __init__(self, idx: int, name: str, ing_text: str, ins_text: str):\n",
    "        self.idx = idx\n",
    "        self.name = name\n",
    "        self.ing_text = ing_text\n",
    "        self.ins_text = ins_text\n",
    "\n",
    "\n",
    "class HybridSearch:\n",
    "    def __init__(self):\n",
    "        self.ing_word = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), min_df=2, strip_accents=\"unicode\")\n",
    "        self.ing_char = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), min_df=2, strip_accents=\"unicode\")\n",
    "        self.ins_word = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), min_df=5, strip_accents=\"unicode\")\n",
    "        self.ins_char = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), min_df=5, strip_accents=\"unicode\")\n",
    "\n",
    "        self.records: List[RecipeRecord] = []\n",
    "        self.M_ing_word = None\n",
    "        self.M_ing_char = None\n",
    "        self.M_ins_word = None\n",
    "        self.M_ins_char = None\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> \"HybridSearch\":\n",
    "        ing_norm = df[\"ingredients\"].apply(normalize_ingredients)\n",
    "        ins_norm = df[\"instructions\"].apply(normalize_instructions)\n",
    "\n",
    "        self.records = []\n",
    "        for i in range(len(df)):\n",
    "            self.records.append(RecipeRecord(i, str(df.iloc[i][\"name\"]), ing_norm.iloc[i], ins_norm.iloc[i]))\n",
    "\n",
    "        ing_corpus = [r.ing_text for r in self.records]\n",
    "        ins_corpus = [r.ins_text for r in self.records]\n",
    "\n",
    "        self.M_ing_word = self.ing_word.fit_transform(ing_corpus)\n",
    "        self.M_ing_char = self.ing_char.fit_transform(ing_corpus)\n",
    "\n",
    "        if any(len(t) > 0 for t in ins_corpus):\n",
    "            self.M_ins_word = self.ins_word.fit_transform(ins_corpus)\n",
    "            self.M_ins_char = self.ins_char.fit_transform(ins_corpus)\n",
    "        return self\n",
    "\n",
    "    def _encode_query(self, query_ingredients: Union[str, List[str]]) -> Tuple:\n",
    "        if isinstance(query_ingredients, list):\n",
    "            q_text = \" \".join(tokenize_items(query_ingredients))\n",
    "        else:\n",
    "            q_text = \" \".join(tokenize_items(query_ingredients))\n",
    "        q_ing_word = self.ing_word.transform([q_text])\n",
    "        q_ing_char = self.ing_char.transform([q_text])\n",
    "        q_ins_word = self.ins_word.transform([q_text]) if self.M_ins_word is not None else None\n",
    "        q_ins_char = self.ins_char.transform([q_text]) if self.M_ins_char is not None else None\n",
    "        return q_ing_word, q_ing_char, q_ins_word, q_ins_char\n",
    "\n",
    "    def search(self, query_ingredients: Union[str, List[str]], top_k: int = 5) -> List[Tuple[float, RecipeRecord]]:\n",
    "        q_ing_word, q_ing_char, q_ins_word, q_ins_char = self._encode_query(query_ingredients)\n",
    "\n",
    "        s_ing_word = linear_kernel(q_ing_word, self.M_ing_word)[0]\n",
    "        s_ing_char = linear_kernel(q_ing_char, self.M_ing_char)[0]\n",
    "        s_ing = 0.6 * s_ing_word + 0.4 * s_ing_char\n",
    "\n",
    "        if self.M_ins_word is not None and q_ins_word is not None:\n",
    "            s_ins_word = linear_kernel(q_ins_word, self.M_ins_word)[0]\n",
    "            s_ins_char = linear_kernel(q_ins_char, self.M_ins_char)[0]\n",
    "            s_ins = 0.6 * s_ins_word + 0.4 * s_ins_char\n",
    "        else:\n",
    "            s_ins = np.zeros_like(s_ing)\n",
    "\n",
    "        sim = 0.75 * s_ing + 0.25 * s_ins\n",
    "\n",
    "        top_idx = np.argsort(sim)[::-1][:top_k]\n",
    "        return [(float(sim[i]), self.records[i]) for i in top_idx]\n",
    "\n",
    "# =========================\n",
    "# Evaluation\n",
    "# =========================\n",
    "def evaluate_model(model: HybridSearch, top_k: int = 5, num_queries: int = 200, seed: int = RANDOM_SEED) -> Dict[str, float]:\n",
    "    rng = random.Random(seed)\n",
    "    candidates = [r for r in model.records if len(r.ing_text.split()) >= 3]\n",
    "    sample = rng.sample(candidates, k=min(num_queries, len(candidates)))\n",
    "\n",
    "    hit1 = hitk = 0\n",
    "    rr_sum = prec_sum = rec_sum = 0.0\n",
    "\n",
    "    for r in sample:\n",
    "        toks = r.ing_text.split()\n",
    "        q_len = min(len(toks), rng.randint(3, min(8, len(toks))))\n",
    "        q = \" \".join(rng.sample(toks, q_len))\n",
    "\n",
    "        results = model.search(q, top_k=top_k)\n",
    "        ranks = [i for i, (_, rec) in enumerate(results, start=1) if rec.idx == r.idx]\n",
    "        if ranks:\n",
    "            rank = ranks[0]\n",
    "            if rank == 1:\n",
    "                hit1 += 1\n",
    "            if rank <= top_k:\n",
    "                hitk += 1\n",
    "                rr_sum += 1.0 / rank\n",
    "                prec_sum += 1.0 / top_k\n",
    "                rec_sum += 1.0\n",
    "\n",
    "    n = max(1, len(sample))\n",
    "    return {\n",
    "        \"n_queries\": float(n),\n",
    "        \"top1_acc\": hit1 / n,\n",
    "        f\"top{top_k}_acc\": hitk / n,\n",
    "        f\"mrr@{top_k}\": rr_sum / n,\n",
    "        f\"precision@{top_k}\": prec_sum / n,\n",
    "        f\"recall@{top_k}\": rec_sum / n,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# GUI\n",
    "# =========================\n",
    "def start_gui(model: HybridSearch):\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Recipe Search (Hybrid TF-IDF)\")\n",
    "\n",
    "    tk.Label(root, text=\"Enter Ingredients (comma-separated):\").pack(pady=(8, 2))\n",
    "    entry = tk.Entry(root, width=80)\n",
    "    entry.pack(padx=8, pady=(0, 6))\n",
    "\n",
    "    output = scrolledtext.ScrolledText(root, width=100, height=24)\n",
    "    output.pack(padx=8, pady=6)\n",
    "\n",
    "    def search_action():\n",
    "        query = entry.get().strip()\n",
    "        if not query:\n",
    "            messagebox.showwarning(\"Input Error\", \"Please enter some ingredients.\")\n",
    "            return\n",
    "        results = model.search(query, top_k=TOP_K_DEFAULT)\n",
    "        output.delete(1.0, tk.END)\n",
    "        for i, (score, rec) in enumerate(results, 1):\n",
    "            output.insert(tk.END, f\"{i}. {rec.name}  (score: {score:.3f})\\n\")\n",
    "            output.insert(tk.END, f\"   Ingredients (norm): {rec.ing_text[:220]}{'...' if len(rec.ing_text)>220 else ''}\\n\")\n",
    "            if rec.ins_text:\n",
    "                output.insert(tk.END, f\"   Steps (norm): {rec.ins_text[:220]}{'...' if len(rec.ins_text)>220 else ''}\\n\")\n",
    "            output.insert(tk.END, \"\\n\")\n",
    "\n",
    "    tk.Button(root, text=\"Search Recipes\", command=search_action).pack(pady=(0, 10))\n",
    "    root.mainloop()\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    df_all = load_all_datasets(DATASETS)\n",
    "    model = HybridSearch().fit(df_all)\n",
    "\n",
    "    metrics = evaluate_model(model, top_k=TOP_K_DEFAULT, num_queries=200)\n",
    "    print(\"\\nEvaluation (synthetic retrieval):\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"  {k:>14}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {k:>14}: {v}\")\n",
    "\n",
    "    start_gui(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f1900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
